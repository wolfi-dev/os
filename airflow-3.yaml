package:
  name: airflow-3
  version: "3.0.0"
  epoch: 0
  description: Platform to programmatically author, schedule, and monitor workflows
  options:
    #  There is a dependency on libarrow.so although it
    #  is provided in the virtual environment. Enabling no-depends
    #  works around this
    no-depends: true
  dependencies:
    runtime:
      - coreutils # numfmt is required
      - merged-usrsbin
      - netcat-openbsd # nc is required
      - posix-libc-utils # genconf is required
      - python-${{vars.py-version}}
      - tzdata
      - unixodbc
      - wolfi-baselayout
  copyright:
    - license: Apache-2.0

# As of now airflow is primarily available for python 3.12 and will break for python 3.13
# For ref: https://airflow.apache.org/blog/airflow-2.9.0
vars:
  py-version: 3.12

environment:
  contents:
    packages:
      - cargo-auditable
      - findutils
      - gcc
      - glibc-dev
      - glibc-locales
      - localedef
      - mariadb-connector-c-dev
      - mariadb-dev
      - nodejs
      - npm
      - pkgconf-dev
      - pnpm
      - postgresql-dev
      - py${{vars.py-version}}-build-bin
      - py${{vars.py-version}}-pip
      - py${{vars.py-version}}-pip
      - py${{vars.py-version}}-xmlsec
      # Airflow requires python<3.13
      - python-${{vars.py-version}}
      - python-${{vars.py-version}}-dev
      - rust
      - wolfi-base
      - xmlsec-dev
      - xmlsec-openssl
      - yarn

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/apache/airflow
      tag: ${{package.version}}
      expected-commit: 40cb51d135094c6eca7c69e75fb192f9d1113bde

  - runs: |
      # By default, the airflow celery provider is not built, but running the upstream helm chart requires it
      #python patch.py # WIP
      # remove the patcher
      rm -rf patch.py

  - uses: patch
    with:
      patches: urllib3.patch

  - working-directory: ./airflow-core/src/airflow/ui
    runs: |
      # front-end build
      yarn install --frozen-lockfile
      # Update BrowsersList regularly: https://github.com/browserslist/update-db#readme
      npx update-browserslist-db@latest
      yarn run build
      rm -rf node_modules

  - runs: |
      # requires EPOCH to be later that 1980
      export SOURCE_DATE_EPOCH=315532800

      # To install mysqlclient wheel
      export MYSQLCLIENT_CFLAGS=`mysql_config --cflags`
      export MYSQLCLIENT_CFLAGS=`mysql_config --cflags`

      python${{vars.py-version}} -m build --wheel
      pip${{vars.py-version}} install --verbose \
       --prefix="/opt/airflow" --root=${{targets.contextdir}} dist/*.whl

  - runs: |
      # CVE-2024-6345 GHSA-cx63-2mw6-8hw5
      # setuptools comes from airflow/providers/google/provider.yaml having
      # gcloud-aio-auth>=4.0.0,<5.0.0 . gcloud-aio-auth 4 is backlevel and has
      # setuptools in it's pyproject.toml 'tool.poetry.dependencies'
      # The tldr; For that case it is not needed in runtime.
      pip${{vars.py-version}} uninstall --yes setuptools

      #GHSA-8w49-h785-mj3c/GHSA-8495-4g3g-x7pr/GHSA-27mf-ghqm-j3j8 fixes
      pip${{vars.py-version}} install --verbose \
        --force-reinstall --prefix=/opt/airflow --root="${{targets.contextdir}}" \
        aiohttp>=3.10.11 tornado>=6.4.2 apache-airflow-providers-celery==3.10.0 statsd packaging pyparsing
      # NOTE: apache-airflow-providers-celery is pinned to 3.10.0 because 3.10.3 introduced a regresssion.
      # https://github.com/apache/airflow/issues/47781 details more about it.
      # before removing the pinning, please check if the images are coming good with helm charts.

  - runs: find . -name '__pycache__' -exec rm -rf {} +

  - runs: |
      mkdir -p ${{targets.destdir}}/opt/airflow/dags
      mkdir -p ${{targets.destdir}}/opt/airflow/logs
      mkdir -p ${{targets.destdir}}/scripts/docker

      # The first time you run Airflow, it will create a file called `airflow.cfg` in
      # `$AIRFLOW_HOME` directory
      # However, for production case it is advised to generate the configuration
      export PYTHONPATH=${{targets.destdir}}/opt/airflow/lib/python${{vars.py-version}}/site-packages

      ${{targets.destdir}}/opt/airflow/bin/airflow config list --defaults > ${{targets.destdir}}/"airflow.cfg"

      cp airflow-core/src/airflow/config_templates/default_webserver_config.py ${{targets.contextdir}}/

      install -Dm755 scripts/docker/entrypoint_prod.sh ${{targets.destdir}}/entrypoint
      install -Dm755 scripts/docker/clean-logs.sh ${{targets.destdir}}/clean-logs
      install -Dm755 scripts/docker/airflow-scheduler-autorestart.sh ${{targets.destdir}}/airflow-scheduler-autorestart
      install -Dm755 scripts/docker/* ${{targets.destdir}}/scripts/docker

subpackages:
  - name: airflow-3-compat
    dependencies:
      runtime:
        - ${{package.name}}
        - merged-usrsbin
        - wolfi-baselayout
    pipeline:
      - runs: |
          # Symlink libstdc++ from usr/lib to /usr/lib/$(uname -m)-linux-gnu/
          mkdir -p ${{targets.subpkgdir}}/usr/lib/$(uname -m)-linux-gnu
          ln -sf /usr/lib/libstdc++.so.6 ${{targets.subpkgdir}}/usr/lib/$(uname -m)-linux-gnu/

  - name: airflow-3-bitnami-compat
    dependencies:
      runtime:
        - ${{package.name}}
        - bash
        - busybox
        - coreutils
        - ini-file
        - merged-usrsbin
        - wait-for-port
        - wolfi-baselayout
    description: "Compatibility package for Bitnami's Airflow image"
    pipeline:
      - uses: bitnami/compat
        with:
          image: airflow
          version-path: 2/debian-12
      - runs: |
          mkdir -p ${{targets.subpkgdir}}/opt/bitnami
          ln -sf /opt/airflow ${{targets.subpkgdir}}/opt/bitnami
          mkdir -p ${{targets.subpkgdir}}/opt/airflow/venv
          chmod g+rwX ${{targets.subpkgdir}}/opt/bitnami
          ln -sf /opt/airflow/lib /opt/airflow/bin ${{targets.subpkgdir}}/opt/airflow/venv
          mkdir -p ${{targets.subpkgdir}}/opt/airflow/bin
          touch ${{targets.subpkgdir}}/opt/airflow/bin/activate
          chmod 0755 ${{targets.subpkgdir}}/opt/airflow/bin/activate
          find ${{targets.subpkgdir}}/opt/bitnami/scripts -type f -exec sed -i 's|/opt/bitnami/scripts/|${{targets.subpkgdir}}/opt/bitnami/scripts/|g' {} +
          sed -i 's/-g "root"//g' ${{targets.subpkgdir}}/opt/bitnami/scripts/airflow/postunpack.sh
          sed -i 's/locale-gen/localedef -i en_US -f UTF-8 en_US.UTF-8/g' ${{targets.subpkgdir}}/opt/bitnami/scripts/locales/add-extra-locales.sh
          ${{targets.subpkgdir}}/opt/bitnami/scripts/airflow/postunpack.sh
          ${{targets.subpkgdir}}/opt/bitnami/scripts/locales/add-extra-locales.sh
          find ${{targets.contextdir}}/opt/bitnami -type f -exec sed -E 's#${{targets.contextdir}}##g' -i {} \;
          sed -i '/if ! am_i_root && \[\[ -e "$LIBNSS_WRAPPER_PATH" \]\]; then/,/fi/d' ${{targets.subpkgdir}}/opt/bitnami/scripts/airflow/entrypoint.sh

update:
  enabled: true
  ignore-regex-patterns:
    - 'rc\d+$'
    - 'helm-chart*'
  github:
    identifier: apache/airflow
    tag-filter-prefix: 3.

test:
  environment:
    contents:
      packages:
        - python-${{vars.py-version}}
  pipeline:
    - name: "Test Python package import and version"
      runs: |
        export PATH=/opt/airflow/bin:$PATH
        export PYTHONPATH=/opt/airflow/lib/python${{vars.py-version}}/site-packages
        HOME=/home/build airflow version | grep ${{package.version}}
        python${{vars.py-version}} -c "import airflow"
    - name: "List providers"
      runs: |
        export PATH=/opt/airflow/bin:$PATH
        export PYTHONPATH=/opt/airflow/lib/python${{vars.py-version}}/site-packages

        airflow providers list
    - name: "Load and test a DAG"
      runs: |
        # Set environment variables
        export PATH=/opt/airflow/bin:$PATH
        export PYTHONUSERBASE=/opt/airflow
        export AIRFLOW_HOME=/opt/airflow

        # Prevents warnings about example dags not being in the right place under /opt/airflow
        # This is expected as we intentionally put them under /home/airflow, and we use our own test dag
        export AIRFLOW__CORE__LOAD_EXAMPLES=False

        # Initialize Airflow database
        airflow db migrate

        # Create and test a simple DAG
        echo "Creating and testing a simple DAG..."
        mkdir -p /opt/airflow/dags
        cat <<EOF > /opt/airflow/dags/test_dag.py
        from airflow import DAG
        from airflow.operators.bash import BashOperator
        from datetime import datetime

        with DAG("test_dag", start_date=datetime(2024, 1, 1), schedule=None) as dag:
            task = BashOperator(
                task_id="test_task",
                bash_command="echo Hello World!"
            )
        EOF

        # Reload the db
        airflow dags reserialize

        # List DAGs
        airflow dags list | grep test_dag || (echo "DAG not found!" && exit 1)

        # Test Task Execution in DAG
        airflow tasks test test_dag test_task 2024-01-01 || (echo "Task execution failed!" && exit 1)
    - uses: test/tw/ldd-check
