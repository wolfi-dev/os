package:
  name: airflow-3
  version: "3.1.1"
  epoch: 1 # GHSA-r397-ff8c-wv2g
  description: Platform to programmatically author, schedule, and monitor workflows
  options:
    #  There is a dependency on libarrow.so although it
    #  is provided in the virtual environment. Enabling no-depends
    #  works around this
    no-depends: true
  dependencies:
    runtime:
      - coreutils # numfmt is required
      - graphviz
      - keyutils # mysql provider requires libkeyutils.so.1
      - libudev # for libudev.so.1
      - mariadb-connector-c # for libmariadb.so.3
      - netcat-openbsd # nc is required
      - posix-libc-utils # genconf is required
      - python-${{vars.python-version}}
      - tzdata
      - unixodbc
    provides:
      - airflow=${{package.full-version}}
  copyright:
    - license: Apache-2.0

vars:
  airflow-home: '/opt/airflow'
  # This should be /home/airflow/.local in the future
  venv-home: '/opt/airflow'
  # As of now (2025/04/28), airflow is primarily available for python 3.12 and will break for python 3.13
  # For ref: https://airflow.apache.org/blog/airflow-2.9.0
  # UPDATE: (2025/10/28), python 3.13 is supported as of v3.1.0. Odd timing results from airflow following python EOL cadence
  # URL: https://airflow.apache.org/docs/apache-airflow/stable/release_notes.html#python-3-13-support-added-3-9-support-removed
  python-version: '3.13'
  # This list is based on providers that were part of the 2.x version of this package, as well as manually added dependencies
  # to ensure we're building dependent providers too. This is an ordered list.
  # Use , as a separator
  providers: 'standard,amazon,sqlite,imap,smtp,ftp,fab,http,celery,cncf-kubernetes,docker,elasticsearch,google,grpc,hashicorp,microsoft-azure,mysql,odbc,openlineage,postgres,redis,sendgrid,sftp,slack,snowflake,ssh'

var-transforms:
  - from: ${{package.version}}
    match: ^(\d+)\.\d+\.\d+$
    replace: "$1"
    to: major-version

environment:
  contents:
    packages:
      - build-base
      - busybox
      - curl
      - glibc-locales
      - localedef
      - mariadb-connector-c-dev
      - mariadb-dev
      - pkgconf
      - python-${{vars.python-version}}-dev
      - uv

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/apache/airflow
      tag: ${{package.version}}
      expected-commit: f969e6374daa8469938169be16a28f7c073a5ce9

  - runs: |
      curl -f https://raw.githubusercontent.com/apache/airflow/constraints-${{package.version}}/constraints-${{vars.python-version}}.txt \
           -o constraints-${{vars.python-version}}.txt


  - runs: |
      # ZIP doesn't handle the SDE we set correctly, so set it to 1980-01-01
      export SOURCE_DATE_EPOCH=315532800

      # Dynamically set cflags for mysqlclient
      export MYSQLCLIENT_CFLAGS=`mysql_config --cflags`

      # Create and source virtual environment
      uv venv ${{vars.venv-home}} --system-site-packages
      source ${{vars.venv-home}}/bin/activate

      # Use constraint file to respect upstream dependency versions
      uv pip install --constraint constraints-${{vars.python-version}}.txt apache-airflow[${{vars.providers}}]==${{package.version}}

      # Graphviz is used by Airflow's DAG viewer
      uv pip install --constraint constraints-${{vars.python-version}}.txt graphviz

      # Uninstall pip and virtualenv from virtual environment
      #
      # pip doesn't need to be present in the virtual environment,
      # the venv can be managed via a system installed package manager
      #
      # virtualenv vendors an outdated and unused pip wheel
      uv pip uninstall pip virtualenv

      # Remove litellm log file that generates secret alerts from trivy
      find ${{vars.venv-home}}/lib/python${{vars.python-version}} -name 'litellm.log' -exec rm {} +

      # Replace hooks.slack.com with something obviously not hooks.slack.com to avoid secret alert from trivy
      find ${{vars.venv-home}}/lib/python${{vars.python-version}}/site-packages/litellm -name _types.py -exec sed -i "s/hooks.slack.com/nothooks.slack.com/g" {} +

      # Remove any patch related files from the install tree
      find ${{vars.venv-home}}/lib/python${{vars.python-version}} -name "*.orig" -print -exec rm {} +

      # The first time you run Airflow, it will create a file called `airflow.cfg` in
      # `$AIRFLOW_HOME` directory
      # However, for production case it is advised to generate the configuration
      airflow config list --defaults > ${{targets.destdir}}/airflow.cfg

      # Install
      mkdir -p ${{targets.contextdir}}/opt
      mv ${{vars.venv-home}} ${{targets.contextdir}}/opt/

      chmod 0644 ${{targets.contextdir}}/${{vars.venv-home}}/.lock

      mkdir -p ${{targets.destdir}}/${{vars.airflow-home}}/dags
      mkdir -p ${{targets.destdir}}/${{vars.airflow-home}}/logs
      mkdir -p ${{targets.destdir}}/scripts/docker

      cp airflow-core/src/airflow/config_templates/default_webserver_config.py ${{targets.contextdir}}/

      install -Dm755 scripts/docker/entrypoint_prod.sh ${{targets.destdir}}/entrypoint
      install -Dm755 scripts/docker/clean-logs.sh ${{targets.destdir}}/clean-logs
      install -Dm755 scripts/docker/airflow-scheduler-autorestart.sh ${{targets.destdir}}/airflow-scheduler-autorestart
      install -Dm755 scripts/docker/* ${{targets.destdir}}/scripts/docker

subpackages:
  - name: ${{package.name}}-compat
    options:
      no-depends: true
    dependencies:
      runtime:
        - ${{package.name}}
      provides:
        - airflow-compat=${{package.full-version}}
    pipeline:
      - runs: |
          # Symlink libstdc++ from usr/lib to /usr/lib/$(uname -m)-linux-gnu/
          mkdir -p ${{targets.contextdir}}/usr/lib/$(uname -m)-linux-gnu
          ln -s /usr/lib/libstdc++.so.6 ${{targets.contextdir}}/usr/lib/$(uname -m)-linux-gnu/
    test:
      pipeline:
        - uses: test/virtualpackage
          with:
            virtual-pkg-name: airflow-compat
            real-pkg-name: ${{subpkg.name}}

  - name: ${{package.name}}-iamguarded-compat
    dependencies:
      runtime:
        - ${{package.name}}=${{package.full-version}}
        - bash
        - busybox
        - coreutils
        - ini-file
        - wait-for-port
      provides:
        - airflow-iamguarded-compat=${{package.full-version}}
    description: "Iamguarded compat for Airflow"
    pipeline:
      - uses: iamguarded/build-compat
        with:
          package: airflow
          version: ${{vars.major-version}}
      - runs: |
          mkdir -p /opt/iamguarded
          ln -s ${{vars.venv-home}} /opt/iamguarded
          mkdir -p ${{targets.contextdir}}/${{vars.airflow-home}}/venv
          chmod g+rwX /opt/iamguarded
          ln -s ${{vars.venv-home}} ${{targets.contextdir}}/${{vars.airflow-home}}/venv
          mkdir -p ${{targets.contextdir}}/opt/airflow/bin
          touch ${{targets.contextdir}}/opt/airflow/bin/activate
          chmod 0755 ${{targets.contextdir}}/opt/airflow/bin/activate

          # fix find invocations in scripts to allow copying configuration files
          sed -i -E 's#find "\$AIRFLOW_BASE_DIR"#find "\$AIRFLOW_BASE_DIR/"#g' /opt/iamguarded/scripts/*.sh

          # prevent certain changes as compat is using symlink
          sed -i -E 's/configure_permissions_ownership "|ensure_dir_exists "|chmod -R g\+rwX "\$AIRFLOW_BASE_DIR"/true ; # \0/' \
            /opt/iamguarded/scripts/airflow/postunpack.sh

          sed -i 's/locale-gen/localedef -i en_US -f UTF-8 en_US.UTF-8/g' /opt/iamguarded/scripts/locales/generate-locales.sh

          bash -x /opt/iamguarded/scripts/airflow/postunpack.sh
          bash -x /opt/iamguarded/scripts/locales/generate-locales.sh
          sed -i '/if ! am_i_root && \[\[ -e "$LIBNSS_WRAPPER_PATH" \]\]; then/,/fi/d' /opt/iamguarded/scripts/airflow/entrypoint.sh
      - uses: iamguarded/finalize-compat
        with:
          package: airflow
          version: ${{vars.major-version}}
    test:
      environment:
        accounts:
          groups:
            - groupname: airflow
              gid: 1001
          users:
            - username: airflow
              gid: 1001
              uid: 1001
          run-as: 0
        contents:
          packages:
            - ${{package.name}}
            - sudo
      pipeline:
        - uses: iamguarded/test-compat
          with:
            package: airflow
            version: ${{vars.major-version}}
        - working-directory: /
          pipeline:
            - name: "Launch the entrypoint / run scripts"
              uses: test/daemon-check-output
              with:
                setup: |
                  chown 1001:1001 -fR /opt/airflow
                  mkdir -p /opt/airflow/tmp /opt/airflow/dags
                start: |
                  sudo -uairflow -E env PYTHONUSERBASE=/opt/airflow AIRFLOW_HOME=/opt/airflow PATH=/opt/iamguarded/airflow/bin:$PATH /opt/iamguarded/scripts/airflow/entrypoint.sh /opt/iamguarded/scripts/airflow/run.sh
                timeout: 60
                expected_output: |
                  Initializing Airflow
                  Configuring Airflow URL

update:
  enabled: true
  ignore-regex-patterns:
    - 'rc\d+$'
    - 'helm-chart*'
  github:
    identifier: apache/airflow
    tag-filter-prefix: 3.

test:
  pipeline:
    - name: "Test Python package import and version"
      runs: |
        source ${{vars.venv-home}}/bin/activate
        airflow version | grep ${{package.version}}
        python${{vars.python-version}} -c "import airflow"
    - name: "Test Flask is at the supported version"
      runs: |
        set -e
        source ${{vars.venv-home}}/bin/activate
        python -c "from flask.json import JSONEncoder"
    - name: "List providers"
      runs: |
        source ${{vars.venv-home}}/bin/activate

        PROVIDERS_LIST="$(airflow providers list)"

        PROVIDERS="$(echo ${{vars.providers}} | sed 's/,/ /g')"
        for provider in $PROVIDERS; do
          echo "$PROVIDERS_LIST" | grep -F "apache-airflow-providers-$provider"
        done
    - name: "Load and test a DAG"
      runs: |
        source ${{vars.venv-home}}/bin/activate

        # Set environment variables
        export AIRFLOW_HOME=/opt/airflow

        # Prevents warnings about example dags not being in the right place under /opt/airflow
        # This is expected as we intentionally put them under /home/airflow, and we use our own test dag
        export AIRFLOW__CORE__LOAD_EXAMPLES=False

        # Initialize Airflow database
        airflow db migrate

        # Create and test a simple DAG
        echo "Creating and testing a simple DAG..."
        mkdir -p /opt/airflow/dags
        cat <<EOF > /opt/airflow/dags/test_dag.py
        from airflow import DAG
        from airflow.operators.bash import BashOperator
        from datetime import datetime

        with DAG("test_dag", start_date=datetime(2024, 1, 1), schedule=None) as dag:
            task = BashOperator(
                task_id="test_task",
                bash_command="echo Hello World!"
            )
        EOF

        # Reload the db
        airflow dags reserialize

        # List DAGs
        airflow dags list | grep test_dag || (echo "DAG not found!" && exit 1)

        # Test Task Execution in DAG
        airflow tasks test test_dag test_task 2024-01-01 || (echo "Task execution failed!" && exit 1)
    - uses: test/tw/ldd-check
