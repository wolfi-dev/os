package:
  name: airflow-3
  version: "3.1.0"
  epoch: 1
  description: Platform to programmatically author, schedule, and monitor workflows
  options:
    #  There is a dependency on libarrow.so although it
    #  is provided in the virtual environment. Enabling no-depends
    #  works around this
    no-depends: true
  dependencies:
    runtime:
      - coreutils # numfmt is required
      - graphviz
      - keyutils # mysql provider requires libkeyutils.so.1
      - libudev # for libudev.so.1
      - mariadb-connector-c # for libmariadb.so.3
      - merged-usrsbin
      - netcat-openbsd # nc is required
      - posix-libc-utils # genconf is required
      - py${{vars.python-version}}-packaging # Airflow uses this at runtime for config loading
      - python-${{vars.python-version}}
      - tzdata
      - unixodbc
      - wolfi-baselayout
    provides:
      - airflow=${{package.full-version}}
  copyright:
    - license: Apache-2.0

vars:
  # As of now (2025/04/28), airflow is primarily available for python 3.12 and will break for python 3.13
  # For ref: https://airflow.apache.org/blog/airflow-2.9.0
  python-version: 3.12
  # This list is based on providers that were part of the 2.x version of this package, as well as manually added dependencies
  # to ensure we're building dependent providers too (not pulling from PyPi). This is an ordered list.
  # Use / as a separator
  providers: "standard amazon sqlite imap smtp ftp fab http celery cncf/kubernetes docker elasticsearch google grpc hashicorp microsoft/azure mysql odbc openlineage postgres redis sendgrid sftp slack snowflake ssh"

var-transforms:
  - from: ${{vars.providers}}
    match: "/"
    replace: "-"
    to: providers-with-dash
  - from: ${{package.version}}
    match: ^(\d+)\.\d+\.\d+$
    replace: "$1"
    to: major-version

environment:
  contents:
    packages:
      - build-base
      - cargo-auditable
      - findutils
      - gcc
      - glibc-dev
      - glibc-locales
      - graphviz
      - localedef
      - mariadb-connector-c-dev
      - mariadb-dev
      - nodejs
      - npm
      - pkgconf-dev
      - pnpm
      - postgresql-dev
      - py${{vars.python-version}}-build-bin
      - py${{vars.python-version}}-pip
      - py${{vars.python-version}}-xmlsec
      - python-${{vars.python-version}}
      - python-${{vars.python-version}}-dev
      - rust
      - uv
      - wget
      - wolfi-base
      - xmlsec-dev
      - xmlsec-openssl
      - yarn

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/apache/airflow
      tag: ${{package.version}}
      expected-commit: 54bd5d8cd9f6f477cc83445737614dec81c4323c

  - uses: patch
    with:
      patches: werkzeug.patch

  - working-directory: ./airflow-core/src/airflow/ui
    runs: |
      # front-end build
      yarn install --frozen-lockfile
      # Update BrowsersList regularly: https://github.com/browserslist/update-db#readme
      npx update-browserslist-db@latest
      yarn run build
      rm -rf node_modules

  - name: "Build and install the main package"
    runs: |
      # requires EPOCH to be later that 1980
      export SOURCE_DATE_EPOCH=315532800

      # To install mysqlclient wheel
      export MYSQLCLIENT_CFLAGS=`mysql_config --cflags`

      python${{vars.python-version}} -m build --wheel
      # We need to use uv to avoid `resolution to deep` errors`
      uv pip install --verbose --upgrade --resolution highest --prefix=${{targets.contextdir}}/opt/airflow dist/*.whl

      # We need to add this additionally  because graphviz is an optional dependency https://github.com/apache/airflow/blob/d2c9563605ca6d3dc40ec0fc2fe6853c86641441/airflow-core/pyproject.toml#L157
      uv pip install --verbose --upgrade --prefix=${{targets.contextdir}}/opt/airflow graphviz

      # CVE-2025-50181 CVE-2025-50182
      uv pip install --verbose --upgrade --prefix=${{targets.contextdir}}/opt/airflow urllib3==2.5.0

      # GHSA-9548-qrrj-x5pj
      uv pip install --verbose --upgrade --prefix=${{targets.contextdir}}/opt/airflow aiohttp==3.12.14

      # GHSA-2c2j-9gv5-cj73
      uv pip install --verbose --upgrade --prefix=${{targets.contextdir}}/opt/airflow starlette==0.47.2

      # GHSA-847f-9342-265h
      uv pip install --verbose --upgrade --prefix=${{targets.contextdir}}/opt/airflow h2==4.3.0

  - name: "Build and install the additional provider packages"
    runs: |
      # By default, the airflow celery provider is not built, but running the upstream helm chart requires it
      # There are a few others we want to include by default as well

      cd providers
      for pkg in ${{vars.providers}}; do
        cd "$pkg"

        # Python will sometimes pull a 2.x airflow version, we do not want this.
        if [[ "$pkg" == "celery" ]]; then
            sed -i 's/"apache-airflow>=2.9.0"/"apache-airflow==${{package.version}}"/' pyproject.toml
        fi

        # apache-airflow-providers-mysql links against libcrypto.so.1.1 on arm64, and does not provide the
        # necessary source files to rebuild the package for arm against libcrypto3 (like the x86 version is),
        # so instead we'll just make sure the provider dependency that does this (mysqlclient) is removed,
        # and we only use mysql-connector-python instead.
        if [[ "$pkg" == "mysql" ]]; then
          if [[ "${{build.arch}}" == "aarch64" ]]; then
            sed -i '/mysql-connector-python>=/d' pyproject.toml
          fi
        fi

        # Build and install the wheel
        python${{vars.python-version}} -m build --wheel
        uv pip install --verbose --upgrade --resolution highest --prefix=${{targets.contextdir}}/opt/airflow dist/*.whl
        cd -
      done

  - name: "bump snowflake provider to remediate CVE-2025-50213"
    runs: |
      uv pip install --verbose --upgrade --prefix=${{targets.contextdir}}/opt/airflow apache-airflow-providers-snowflake>=6.4.0

  # Flask >2.2.5 is not supported. Using Flask > 2.2.5 causes the same issue
  # as the one documented here:
  # - https://github.com/spec-first/connexion/issues/1699#issuecomment-1524042812
  # Upstream is working on supporting Flask 2.2.5+, as documented in the issue
  # and PR below:
  # - https://github.com/apache/airflow/issues/52509
  # - https://github.com/apache/airflow/pull/50960
  - name: "Downgrade Flask 2.2.5"
    runs: |
      uv pip install --verbose --upgrade --prefix=${{targets.contextdir}}/opt/airflow Flask==2.2.5

  - runs: find . -name '__pycache__' -exec rm -rf {} +

  - name: "Remove litellm log file that generates secret alerts from trivy"
    runs: |
      find ${{targets.contextdir}}/opt/airflow/lib/python${{vars.python-version}} -name 'litellm.log' -exec rm -f {} +

  - name: "Replace hooks.slack.com with something obviously not hooks.slack.com to avoid secret alert from trivy"
    runs: |
      find ${{targets.contextdir}}/opt/airflow/lib/python${{vars.python-version}}/site-packages/litellm -name _types.py -exec sed -i "s/hooks.slack.com/nothooks.slack.com/g" {} +

  - name: "Remove any patch related files from the install tree"
    runs: |
      find ${{targets.contextdir}}/opt/airflow/lib/python${{vars.python-version}} -name "*.orig" -print -exec rm -f {} +

  - runs: |
      chmod 0644 ${{targets.contextdir}}/opt/airflow/.lock

      mkdir -p ${{targets.destdir}}/opt/airflow/dags
      mkdir -p ${{targets.destdir}}/opt/airflow/logs
      mkdir -p ${{targets.destdir}}/scripts/docker

      # The first time you run Airflow, it will create a file called `airflow.cfg` in
      # `$AIRFLOW_HOME` directory
      # However, for production case it is advised to generate the configuration
      export PYTHONPATH=${{targets.contextdir}}/opt/airflow:${{targets.contextdir}}/opt/airflow/lib/python${{vars.python-version}}/site-packages
      export PATH=${{targets.contextdir}}/opt/airflow/bin:$PATH

      ${{targets.destdir}}/opt/airflow/bin/airflow config list --defaults > ${{targets.destdir}}/"airflow.cfg"

      cp airflow-core/src/airflow/config_templates/default_webserver_config.py ${{targets.contextdir}}/

      install -Dm755 scripts/docker/entrypoint_prod.sh ${{targets.destdir}}/entrypoint
      install -Dm755 scripts/docker/clean-logs.sh ${{targets.destdir}}/clean-logs
      install -Dm755 scripts/docker/airflow-scheduler-autorestart.sh ${{targets.destdir}}/airflow-scheduler-autorestart
      install -Dm755 scripts/docker/* ${{targets.destdir}}/scripts/docker

subpackages:
  - name: ${{package.name}}-compat
    dependencies:
      runtime:
        - ${{package.name}}
        - merged-usrsbin
        - wolfi-baselayout
      provides:
        - airflow-compat=${{package.full-version}}
    pipeline:
      - runs: |
          # Symlink libstdc++ from usr/lib to /usr/lib/$(uname -m)-linux-gnu/
          mkdir -p ${{targets.subpkgdir}}/usr/lib/$(uname -m)-linux-gnu
          ln -sf /usr/lib/libstdc++.so.6 ${{targets.subpkgdir}}/usr/lib/$(uname -m)-linux-gnu/
    test:
      pipeline:
        - uses: test/virtualpackage
          with:
            virtual-pkg-name: airflow-compat
            real-pkg-name: ${{subpkg.name}}

  - name: ${{package.name}}-iamguarded-compat
    dependencies:
      runtime:
        - ${{package.name}}
        - bash
        - busybox
        - coreutils
        - ini-file
        - wait-for-port
        - wolfi-baselayout
      provides:
        - airflow-iamguarded-compat=${{package.full-version}}
    description: "Iamguarded compat for Airflow"
    pipeline:
      - uses: iamguarded/build-compat
        with:
          package: airflow
          version: ${{vars.major-version}}
      - runs: |
          mkdir -p /opt/iamguarded
          ln -sf /opt/airflow /opt/iamguarded
          mkdir -p ${{targets.contextdir}}/opt/airflow/venv
          chmod g+rwX /opt/iamguarded
          ln -sf /opt/airflow/lib /opt/airflow/bin ${{targets.contextdir}}/opt/airflow/venv
          mkdir -p ${{targets.contextdir}}/opt/airflow/bin
          touch ${{targets.contextdir}}/opt/airflow/bin/activate
          chmod 0755 ${{targets.contextdir}}/opt/airflow/bin/activate

          # fix find invocations in scripts to allow copying configuration files
          sed -i -E 's#find "\$AIRFLOW_BASE_DIR"#find "\$AIRFLOW_BASE_DIR/"#g' /opt/iamguarded/scripts/*.sh

          # prevent certain changes as compat is using symlink
          sed -i -E 's/configure_permissions_ownership "|ensure_dir_exists "|chmod -R g\+rwX "\$AIRFLOW_BASE_DIR"/true ; # \0/' \
            /opt/iamguarded/scripts/airflow/postunpack.sh

          sed -i 's/locale-gen/localedef -i en_US -f UTF-8 en_US.UTF-8/g' /opt/iamguarded/scripts/locales/generate-locales.sh

          bash -x /opt/iamguarded/scripts/airflow/postunpack.sh
          bash -x /opt/iamguarded/scripts/locales/generate-locales.sh
          sed -i '/if ! am_i_root && \[\[ -e "$LIBNSS_WRAPPER_PATH" \]\]; then/,/fi/d' /opt/iamguarded/scripts/airflow/entrypoint.sh
      - uses: iamguarded/finalize-compat
        with:
          package: airflow
          version: ${{vars.major-version}}
    test:
      environment:
        accounts:
          groups:
            - groupname: airflow
              gid: 1001
          users:
            - username: airflow
              gid: 1001
              uid: 1001
          run-as: 0
        contents:
          packages:
            - ${{package.name}}
            - sudo
      pipeline:
        - uses: iamguarded/test-compat
          with:
            package: airflow
            version: ${{vars.major-version}}
        - working-directory: /
          pipeline:
            - name: "Launch the entrypoint / run scripts"
              uses: test/daemon-check-output
              with:
                setup: |
                  chown 1001:1001 -fR /opt/airflow
                  mkdir -p /opt/airflow/tmp /opt/airflow/dags
                start: |
                  sudo -uairflow -E env PYTHONUSERBASE=/opt/airflow AIRFLOW_HOME=/opt/airflow PATH=/opt/iamguarded/airflow/bin:$PATH /opt/iamguarded/scripts/airflow/entrypoint.sh /opt/iamguarded/scripts/airflow/run.sh
                timeout: 60
                expected_output: |
                  Initializing Airflow
                  Configuring Airflow URL

update:
  enabled: true
  ignore-regex-patterns:
    - 'rc\d+$'
    - "helm-chart*"
  github:
    identifier: apache/airflow
    tag-filter-prefix: 3.

test:
  environment:
    contents:
      packages:
        - py${{vars.python-version}}-pip
  pipeline:
    - name: "Ensure no airflow 2.x components are pulled in by mistake"
      runs: |
        pip list --path=/opt/airflow/lib/python${{vars.python-version}}/site-packages/ | grep airflow
        ls -lah /opt/airflow/lib/python${{vars.python-version}}/site-packages/

        # A known bad file example: apache_airflow-2.10.5.dist-info
        if ls -1 /opt/airflow/lib/python${{vars.python-version}}/site-packages | grep -q "apache_airflow-2.*.dist-info"; then
            echo "Error: apache_airflow-2.* file(s) found! Please diagnose"
            exit 1
        fi
    - name: "Test Python package import and version"
      runs: |
        export PATH=/opt/airflow/bin:$PATH
        export PYTHONPATH=/opt/airflow/lib/python${{vars.python-version}}/site-packages
        HOME=/home/build airflow version | grep ${{package.version}}
        python${{vars.python-version}} -c "import airflow"
    - name: "Test Flask is at the supported version"
      runs: |
        set -e
        export PYTHONPATH=/opt/airflow/lib/python${{vars.python-version}}/site-packages
        python -c "from flask.json import JSONEncoder"
    - name: "List providers"
      runs: |
        export PATH=/opt/airflow/bin:$PATH
        export PYTHONPATH=/opt/airflow/lib/python${{vars.python-version}}/site-packages

        PROVIDERS_LIST=$(airflow providers list)
        echo $PROVIDERS_LIST

        # Use - as a separator
        for pkg in ${{vars.providers-with-dash}}; do
          echo $PROVIDERS_LIST | grep "apache-airflow-providers-$pkg"
        done
    - name: "Load and test a DAG"
      runs: |
        # Set environment variables
        export PATH=/opt/airflow/bin:$PATH
        export PYTHONUSERBASE=/opt/airflow
        export AIRFLOW_HOME=/opt/airflow

        # Prevents warnings about example dags not being in the right place under /opt/airflow
        # This is expected as we intentionally put them under /home/airflow, and we use our own test dag
        export AIRFLOW__CORE__LOAD_EXAMPLES=False

        # Initialize Airflow database
        airflow db migrate

        # Create and test a simple DAG
        echo "Creating and testing a simple DAG..."
        mkdir -p /opt/airflow/dags
        cat <<EOF > /opt/airflow/dags/test_dag.py
        from airflow import DAG
        from airflow.operators.bash import BashOperator
        from datetime import datetime

        with DAG("test_dag", start_date=datetime(2024, 1, 1), schedule=None) as dag:
            task = BashOperator(
                task_id="test_task",
                bash_command="echo Hello World!"
            )
        EOF

        # Reload the db
        airflow dags reserialize

        # List DAGs
        airflow dags list | grep test_dag || (echo "DAG not found!" && exit 1)

        # Test Task Execution in DAG
        airflow tasks test test_dag test_task 2024-01-01 || (echo "Task execution failed!" && exit 1)
    - uses: test/tw/ldd-check
