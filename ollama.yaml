package:
  name: ollama
  version: "0.9.0"
  epoch: 1
  description: Get up and running with Llama 2 and other large language models locally
  copyright:
    - license: MIT

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/ollama/ollama
      tag: v${{package.version}}
      expected-commit: 5f57b0ef4268a6bd9e8043d54c351a608a7e1bca

  - uses: go/build
    with:
      modroot: .
      packages: .
      output: ollama
      ldflags: |
        -X=github.com/ollama/ollama/version.Version=${{package.version}}
        -X=github.com/ollama/ollama/server.mode=release

subpackages:
  - name: ${{package.name}}-cpu
    description: Ollama CPU inferencing component
    dependencies:
      provides:
        - libggml-base=${{package.full-version}}
        - libggml-cpu=${{package.full-version}}
      runtime:
        - libstdc++
    pipeline:
      - uses: cmake/configure
        with:
          opts: --preset "CPU"
      - uses: cmake/build
      - uses: cmake/install
    test:
      pipeline:
        - uses: test/tw/ldd-check
          with:
            packages: ${{package.name}}-cpu
            extra-library-paths: /usr/lib/ollama

update:
  enabled: true
  github:
    identifier: ollama/ollama
    strip-prefix: v

test:
  environment:
    contents:
      packages:
        - curl
        - wait-for-it
  pipeline:
    - name: Ensure CLI works
      runs: |
        ollama --version | grep "${{package.version}}"
        ollama --help | grep "Large language model"
    - name: Test if Ollama server works
      runs: |
        HOST_IP=localhost
        PREDEFINED_PORT=9999
        OLLAMA_HOST=$HOST_IP:$PREDEFINED_PORT ollama serve > /dev/null 2>&1 &
        SERVER_PID=$!
        wait-for-it -t 10 --strict $HOST_IP:$PREDEFINED_PORT

        # Check if the server is running
        curl -s $HOST_IP:$PREDEFINED_PORT | grep -q "Ollama is running"

        # Kill the server process
        kill $SERVER_PID
