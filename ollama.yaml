package:
  name: ollama
  version: "0.7.0"
  epoch: 1
  description: Get up and running with Llama 2 and other large language models locally
  copyright:
    - license: MIT

environment:
  contents:
    packages:
      - cmake

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/ollama/ollama
      tag: v${{package.version}}
      expected-commit: bd68d3ae50c67ba46ee94a584fa6d0386e4b8522

  - uses: go/build
    with:
      modroot: .
      packages: .
      output: ollama
      ldflags: -X=github.com/ollama/ollama/version.Version=${{package.version}} -linkmode external -extldflags "-static"

  - uses: strip

subpackages:
  - name: ${{package.name}}-cpu
    description: Ollama CPU inferencing component
    dependencies:
      runtime:
        - ${{package.name}}
        - libstdc++
    pipeline:
      - uses: cmake/configure
        with:
          opts:
            --preset "CPU"
      - uses: cmake/build
      - uses: cmake/install
      - name: Build LibGGML CPU
        runs: |
          set -e
          install -Dpm0644 -t ${{ targets.subpkgdir }}/usr/lib/ ${{ targets.subpkgdir }}/usr/lib/ollama/libggml*.so
    test:
      pipeline:
        - uses: test/tw/ldd-check
          with:
            packages: ollama-cpu

update:
  enabled: true
  github:
    identifier: ollama/ollama
    strip-prefix: v

test:
  environment:
    contents:
      packages:
        - curl
        - wait-for-it
  pipeline:
    - name: Ensure CLI works
      runs: |
        ollama --version | grep "${{package.version}}"
        ollama --help | grep "Large language model"
    - name: Test if Ollama server works
      runs: |
        HOST_IP=localhost
        PREDEFINED_PORT=9999
        OLLAMA_HOST=$HOST_IP:$PREDEFINED_PORT ollama serve &
        SERVER_PID=$!
        wait-for-it -t 10 --strict $HOST_IP:$PREDEFINED_PORT

        # Check if the server is running
        curl -s $HOST_IP:$PREDEFINED_PORT | grep -q "Ollama is running"

        # Kill the server process
        kill $SERVER_PID
