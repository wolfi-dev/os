package:
  name: spark-3.5-scala-2.13
  version: 3.5.4
  epoch: 32
  description: Unified engine for large-scale data analytics
  copyright:
    - license: Apache-2.0
  dependencies:
    runtime:
      - bash
      - busybox
      - posix-libc-utils
      - procps
      - tini
      - tini-compat

environment:
  contents:
    packages:
      - bash
      - busybox
      - ca-certificates-bundle
      - curl
      - glibc-iconv
      - glibc-locale-en
      - grep
      - maven
      - openjdk-17
      - perl-utils
      - procps
      - py3-setuptools
      - py3.11-pip
      - python-3.13
      - wolfi-base
      - wolfi-baselayout
      - yaml-dev
  environment:
    LANG: en_US.UTF-8
    JAVA_HOME: /usr/lib/jvm/java-17-openjdk
    MAVEN_OPTS: "-Xss64m -Xmx2g -XX:ReservedCodeCacheSize=1g"
    APACHE_MIRROR: "https://repo.maven.apache.org/maven2/org"

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/apache/spark
      tag: v${{package.version}}
      expected-commit: a6f220d951742f4074b37772485ee0ec7a774e7d

  # These are not CVE patches, but patches to make the build work.
  - uses: patch
    with:
      patches: make-distribution.patch mvn.patch GHSA-8qv5-68g4-248j-fix.patch GHSA-4g8c-wm8x-jfhw-fix.patch

  - runs: |
      ./dev/change-scala-version.sh 2.13

  - uses: maven/pombump
    with:
      properties-file: pombump-properties.yaml

  - runs: |
      mkdir -p ${{targets.contextdir}}/usr/lib/spark/
      ./dev/make-distribution.sh --name pyspark-2.13 --pip --tgz -Pscala-2.13 -Phive -Phive-thriftserver -Pyarn
      mv dist/* ${{targets.contextdir}}/usr/lib/spark/
      cp resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh ${{targets.contextdir}}/usr/lib/spark/

subpackages:
  - name: pyspark-scala-2.13
    description: Pyspark built with scala-2.13
    pipeline:
      - working-directory: python
        pipeline:
          - uses: python/build
          - uses: python/install

  - name: ${{package.name}}-compat
    description: "Compatibility package to place binaries in the location expected by upstream image"
    pipeline:
      - runs: |
          mkdir -p "${{targets.subpkgdir}}"/usr/bin
          mkdir -p "${{targets.subpkgdir}}"/opt
      - runs: |
          ln -s /usr/lib/spark/ ${{targets.subpkgdir}}/opt/spark
          ln -sf /usr/lib/spark/bin/spark-submit ${{targets.subpkgdir}}/usr/bin/spark-submit
          ln -sf /usr/lib/spark/bin/spark-shell ${{targets.subpkgdir}}/usr/bin/spark-shell
          ln -sf /usr/lib/spark/bin/spark-class ${{targets.subpkgdir}}/usr/bin/spark-class
          ln -sf /usr/lib/spark/bin/spark-sql ${{targets.subpkgdir}}/usr/bin/spark-sql
          ln -sf /usr/lib/spark/entrypoint.sh ${{targets.subpkgdir}}/opt/entrypoint.sh
      - uses: strip

test:
  environment:
    contents:
      packages:
        - openjdk-17-default-jvm
        - bash
        - python-3.13
    environment:
      LANG: en_US.UTF-8
      SCALA_VERSION: 2.13
      JAVA_HOME: /usr/lib/jvm/java-17-openjdk
      SPARK_LOCAL_IP: 127.0.0.1
      SPARK_LOCAL_HOSTNAME: localhost
      SPARK_HOME: /usr/lib/spark
      SPARK_USER: test
  pipeline:
    - name: Test ${{package.name}} with OpenJDK 17
      pipeline:
        - name: Test if the Scala versions are correct
          runs: ls /usr/lib/spark/jars/scala-* | grep -q $SCALA_VERSION
        - name: Check spark-shell --version
          runs: /usr/lib/spark/bin/spark-shell --version
        - name: Check spark-submit --version
          runs: /usr/lib/spark/bin/spark-submit --version
        - name: Check pyspark --version
          runs: /usr/lib/spark/bin/pyspark --version
        - name: Check spark-sql --version
          runs: /usr/lib/spark/bin/spark-sql --version
        - name: Test entrypoint
          runs: timeout 10 /usr/lib/spark/bin/spark-shell > spark_log.txt 2>&1 || [ $? -eq 143 ] && grep "Spark session available" spark_log.txt && exit_code=0 || exit_code=$? && echo $exit_code
        - name: Run a simple Scala test script
          runs: |
            cat <<EOF > SimpleJob.scala
            val data = Seq(1, 2, 3, 4, 5)
            val rdd = spark.sparkContext.parallelize(data)
            val sum = rdd.reduce(_ + _)
            assert(sum == 15)
            EOF
            cat SimpleJob.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy --master local[*]
        - name: Perform SQL query on DataFrame in Scala
          runs: |
            cat <<EOF > SQLTest.scala
            import org.apache.spark.sql.SparkSession
            val spark = SparkSession.builder.appName("SQLTest").getOrCreate()
            import spark.implicits._
            val df = Seq((1, "Alice"), (2, "Bob")).toDF("id", "name")
            df.createOrReplaceTempView("people")
            val result = spark.sql("SELECT name FROM people WHERE id = 2")
            assert(result.count() == 1 && result.first().getString(0) == "Bob")
            EOF
            cat SQLTest.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy --master local[*]
        - name: Run a simple Spark job in Python
          runs: |
            cat <<EOF > /tmp/simple_job.py
            from pyspark.sql import SparkSession
            spark = SparkSession.builder.appName("SimpleJob").getOrCreate()
            data = [1, 2, 3, 4, 5]
            rdd = spark.sparkContext.parallelize(data)
            sum = rdd.reduce(lambda x, y: x + y)
            assert sum == 15
            EOF
            /usr/lib/spark/bin/spark-submit --conf spark.jars.ivy=/tmp/.ivy --master local[*] /tmp/simple_job.py

update:
  enabled: true
  github:
    identifier: apache/spark
    use-tag: true
    strip-prefix: v
    tag-filter: v3.5
