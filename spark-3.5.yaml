### The main package spark-3.5 is not to be used directly. Instead, use the subpackages.
package:
  name: spark-3.5
  version: "3.5.5"
  epoch: 1
  description: Unified engine for large-scale data analytics
  copyright:
    - license: Apache-2.0

environment:
  contents:
    packages:
      - R
      - R-dev
      - bash
      - busybox
      - ca-certificates-bundle
      - curl
      - glibc-iconv
      - glibc-locale-en
      - grep
      - maven
      - openjdk-11
      - openjdk-17
      - openjdk-8
      - perl-utils
      - procps
      - py3-setuptools
      - py3.11-pip
      - python-3.11
      - wolfi-base
      - wolfi-baselayout
      - yaml-dev
  environment:
    LANG: en_US.UTF-8
    APACHE_MIRROR: "https://repo.maven.apache.org/maven2/org"
    JAVA_HOME: /usr/lib/jvm/java-1.8-openjdk

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/apache/spark
      tag: v${{package.version}}
      expected-commit: 7c29c664cdc9321205a98a14858aaf8daaa19db2

  - uses: patch
    with:
      patches: make-distribution.patch internal-access.patch mvn.patch

  - uses: maven/pombump

data:
  - name: openjdk-versions
    items:
      8: "java-1.8-openjdk"
      11: "java-11-openjdk"
      17: "java-17-openjdk"

subpackages:
  - range: openjdk-versions
    name: ${{package.name}}-scala-2.12-openjdk-${{range.key}}
    dependencies:
      # Both `python` and `openjdk-{11,17}-default-jvm` are required for running Spark,
      # but will be added in the image, since upstream supports different Python versions
      # and Java versions. So we don't need to specify them here to avoid conflicts.
      runtime:
        - openjdk-${{range.key}}-default-jvm
        - bash
        - busybox
        - procps
        - posix-libc-utils
        - tini
        - tini-compat
        - net-tools
        - logrotate
        - linux-pam
        - procps
        - libnss
      provides:
        - spark=${{package.full-version}}
        - ${{package.name}}=${{package.full-version}}
    pipeline:
      - runs: |
          export JAVA_HOME="/usr/lib/jvm/${{range.value}}"
          export PATH=$PATH:$JAVA_HOME/bin

          ./dev/make-distribution.sh --name custom-spark --pip -Psparkr -Phive -Phive-thriftserver -Pmesos -Pyarn -Pkubernetes

          patch dist/bin/load-spark-env.sh load-spark-env.sh.diff
          patch dist/sbin/spark-daemon.sh spark-daemon.sh.diff

          mkdir -p ${{targets.contextdir}}/usr/lib/spark
          mkdir -p ${{targets.contextdir}}/usr/lib/spark/work-dir
          mv dist/kubernetes/dockerfiles/spark/entrypoint.sh ${{targets.contextdir}}/usr/lib/spark
          mv dist/* ${{targets.contextdir}}/usr/lib/spark/
    test:
      environment:
        contents:
          packages:
            - python3
            - openjdk-${{range.key}}-default-jvm
            - R
        environment:
          LANG: en_US.UTF-8
          JAVA_HOME: /usr/lib/jvm/${{range.value}}
          SPARK_LOCAL_IP: 127.0.0.1
          SPARK_LOCAL_HOSTNAME: localhost
          SPARK_HOME: /usr/lib/spark
          SPARK_USER: test
      pipeline:
        - name: Test ${{package.name}} with OpenJDK ${{range.key}}
          pipeline:
            - name: Check spark-shell --version
              runs: /usr/lib/spark/bin/spark-shell --version
            - name: Check spark-submit --version
              runs: /usr/lib/spark/bin/spark-submit --version
            - name: Check pyspark --version
              runs: /usr/lib/spark/bin/pyspark --version
            - name: Check spark-sql --version
              runs: /usr/lib/spark/bin/spark-sql --version
            - name: Test entrypoint
              runs: timeout 10 /opt/entrypoint.sh /opt/spark/bin/spark-shell > spark_log.txt 2>&1 || [ $? -eq 143 ] && grep "Spark session available" spark_log.txt && exit_code=0 || exit_code=$? && echo $exit_code
            - name: Run a simple Scala test script
              runs: |
                cat <<EOF > SimpleJob.scala
                val data = Seq(1, 2, 3, 4, 5)
                val rdd = spark.sparkContext.parallelize(data)
                val sum = rdd.reduce(_ + _)
                assert(sum == 15)
                EOF
                cat SimpleJob.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy --master local[*]
            - name: Run a simple Spark job in Python
              runs: |
                cat <<EOF > simple_job.py
                from pyspark.sql import SparkSession
                spark = SparkSession.builder.appName("SimpleJob").getOrCreate()
                data = [1, 2, 3, 4, 5]
                rdd = spark.sparkContext.parallelize(data)
                sum = rdd.reduce(lambda x, y: x + y)
                assert sum == 15
                EOF
                /usr/lib/spark/bin/spark-submit --conf spark.jars.ivy=/tmp/.ivy --master local[*] simple_job.py --jars /usr/lib/spark/jars/guava-32.0.1-jre.jar
            - name: Perform SQL query on DataFrame in Scala
              runs: |
                cat <<EOF > SQLTest.scala
                import org.apache.spark.sql.SparkSession
                val spark = SparkSession.builder.appName("SQLTest").getOrCreate()
                import spark.implicits._
                val df = Seq((1, "Alice"), (2, "Bob")).toDF("id", "name")
                df.createOrReplaceTempView("people")
                val result = spark.sql("SELECT name FROM people WHERE id = 2")
                assert(result.count() == 1 && result.first().getString(0) == "Bob")
                EOF
                cat SQLTest.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy --master local[*]
            - name: Run basic SQL query with spark-sql
              runs: |
                # Ensure the user environment is properly configured
                if ! whoami &>/dev/null; then
                  echo "sparkuser:x:$(id -u):$(id -g):Spark User:/home/sparkuser:/bin/bash" >> /etc/passwd
                  mkdir -p /home/sparkuser
                  chown $(id -u):$(id -g) /home/sparkuser
                fi

                # Export necessary environment variables
                export USER=$(whoami)
                export LOGNAME=$(whoami)
                export HADOOP_USER_NAME=$(whoami)

                # Create a basic SQL query
                echo 'CREATE OR REPLACE TEMP VIEW test AS SELECT 1 AS id;' > test.sql
                echo 'SELECT * FROM test;' >> test.sql

                # Run the Spark SQL query
                /usr/lib/spark/bin/spark-sql -f test.sql --master local[*] --conf spark.jars.ivy=/tmp/.ivy
            - name: Run SparkR script
              runs: |
                cat <<EOF > sparkr_test.R
                library(SparkR)
                sparkR.session()
                df <- as.DataFrame(data.frame(id = c(1, 2), name = c("Alice", "Bob")))
                createOrReplaceTempView(df, "people")
                df2 <- sql("SELECT * FROM people WHERE id > 1")
                stopifnot(count(df2) == 1)
                EOF
                /usr/lib/spark/bin/spark-submit sparkr_test.R

  - name: ${{package.name}}-scala-2.12-compat
    description: "Compatibility package to place binaries in the location expected by upstream image"
    dependencies:
      provides:
        - spark-compat=${{package.full-version}}
    pipeline:
      - runs: |
          mkdir -p "${{targets.subpkgdir}}"/usr/bin
          mkdir -p "${{targets.subpkgdir}}"/opt/
          ln -s /usr/lib/spark/ ${{targets.subpkgdir}}/opt/spark
          ln -sf /usr/lib/spark/entrypoint.sh ${{targets.subpkgdir}}/opt/entrypoint.sh
          ln -sf /usr/lib/spark/kubernetes/dockerfiles/spark/decom.sh ${{targets.subpkgdir}}/opt/decom.sh
          ln -sf /usr/lib/spark/bin/spark-submit ${{targets.subpkgdir}}/usr/bin/spark-submit
          ln -sf /usr/lib/spark/bin/spark-shell ${{targets.subpkgdir}}/usr/bin/spark-shell
          ln -sf /usr/lib/spark/bin/pyspark ${{targets.subpkgdir}}/usr/bin/pyspark
          ln -sf /usr/lib/spark/bin/spark-class ${{targets.subpkgdir}}/usr/bin/spark-class
          ln -sf /usr/lib/spark/bin/spark-sql ${{targets.subpkgdir}}/usr/bin/spark-sql
          ln -sf /usr/lib/spark/bin/sparkR ${{targets.subpkgdir}}/usr/bin/spark
      - uses: strip

  - range: openjdk-versions
    name: ${{package.name}}-minimal-openjdk-${{range.key}}
    dependencies:
      runtime:
        - openjdk-${{range.key}}-default-jvm
      provides:
        - spark-minimal=${{package.full-version}}
        - ${{package.name}}-minimal=${{package.full-version}}
    pipeline:
      - runs: |
          export JAVA_HOME="/usr/lib/jvm/${{range.value}}"
          export PATH=$PATH:$JAVA_HOME/bin

          ./dev/make-distribution.sh --name minimal-spark --pip -Psparkr -Phive

          mkdir -p ${{targets.contextdir}}/usr/lib/spark
          mv dist/* ${{targets.contextdir}}/usr/lib/spark/

  - name: ${{package.name}}-scala-2.12-bitnami-compat
    description: Bitnami compat for spark 3.5
    dependencies:
      runtime:
        - coreutils # needed for cp -nr command
    pipeline:
      - uses: bitnami/compat
        with:
          image: spark
          version-path: 3.5/debian-12
      - runs: |
          mkdir -p ${{targets.subpkgdir}}/opt/bitnami/spark/
          mkdir -p "${{targets.subpkgdir}}"/opt/bitnami/spark/logs
          mkdir -p "${{targets.subpkgdir}}"/opt/bitnami/spark/tmp

          mkdir -p ${{targets.subpkgdir}}/opt/bitnami/spark/conf.default/
          mkdir -p ${{targets.subpkgdir}}/opt/bitnami/spark/conf/

          cp -r /home/build/melange-out/${{package.name}}-scala-2.12-openjdk-17/usr/lib/spark/conf/* ${{targets.subpkgdir}}/opt/bitnami/spark/conf.default/
          # We copy conf instead of symlinking to avoid issues with the bitnami volume mount.
          cp -r /home/build/melange-out/${{package.name}}-scala-2.12-openjdk-17/usr/lib/spark/conf/* ${{targets.subpkgdir}}/opt/bitnami/spark/conf/

          ln -s /usr/lib/spark/jars ${{targets.subpkgdir}}/opt/bitnami/spark/jars
          ln -s /usr/lib/spark/bin ${{targets.subpkgdir}}/opt/bitnami/spark/bin
          ln -s /usr/lib/spark/data ${{targets.subpkgdir}}/opt/bitnami/spark/data
          ln -s /usr/lib/spark/python ${{targets.subpkgdir}}/opt/bitnami/spark/python
          ln -s /usr/lib/spark/licenses ${{targets.subpkgdir}}/opt/bitnami/spark/licenses
          ln -s /usr/lib/spark/kubernetes ${{targets.subpkgdir}}/opt/bitnami/spark/kubernetes
          ln -s /usr/lib/spark/sbin ${{targets.subpkgdir}}/opt/bitnami/spark/sbin
          ln -s /usr/lib/spark/yarn ${{targets.subpkgdir}}/opt/bitnami/spark/yarn

  - range: openjdk-versions
    name: ${{package.name}}-scala-2.13-openjdk-${{range.key}}
    dependencies:
      runtime:
        - openjdk-${{range.key}}-default-jvm
    pipeline:
      - runs: |
          git stash
          mvn clean
      - uses: patch
        with:
          patches: make-2.13-distribution.patch internal-access.patch mvn.patch GHSA-8qv5-68g4-248j-fix.patch GHSA-4g8c-wm8x-jfhw-fix.patch
      - runs: |
          export JAVA_HOME="/usr/lib/jvm/${{range.value}}"
          export PATH=$PATH:$JAVA_HOME/bin
          ./dev/change-scala-version.sh 2.13
      - uses: maven/pombump
        with:
          properties-file: pombump-properties-2.13.yaml
      - runs: |
          mkdir -p ${{targets.contextdir}}/usr/lib/spark/
          ./dev/make-distribution.sh --name pyspark-2.13 --pip --tgz -Pscala-2.13 -Phive -Phive-thriftserver -Pyarn
          mv dist/* ${{targets.contextdir}}/usr/lib/spark/
          cp resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh ${{targets.contextdir}}/usr/lib/spark/
    test:
      environment:
        contents:
          packages:
            - openjdk-${{range.key}}-default-jvm
            - bash
            - python-3.11
            - pyspark-scala-2.13
        environment:
          LANG: en_US.UTF-8
          SCALA_VERSION: 2.13
          JAVA_HOME: /usr/lib/jvm/${{range.value}}
          SPARK_LOCAL_IP: 127.0.0.1
          SPARK_LOCAL_HOSTNAME: localhost
          SPARK_HOME: /usr/lib/spark
          SPARK_USER: test
      pipeline:
        - name: Test ${{package.name}} with different versions of OpenJDK
          pipeline:
            - name: Test if the Scala versions are correct
              runs: ls /usr/lib/spark/jars/scala-* | grep -q $SCALA_VERSION
            - name: Check spark-shell --version
              runs: /usr/lib/spark/bin/spark-shell --version
            - name: Check spark-submit --version
              runs: /usr/lib/spark/bin/spark-submit --version
            - name: Check pyspark --version
              runs: /usr/lib/spark/bin/pyspark --version
            - name: Check spark-sql --version
              runs: /usr/lib/spark/bin/spark-sql --version
            - name: Test entrypoint
              runs: timeout 10 /usr/lib/spark/bin/spark-shell > spark_log.txt 2>&1 || [ $? -eq 143 ] && grep "Spark session available" spark_log.txt && exit_code=0 || exit_code=$? && echo $exit_code
            - name: Run a simple Scala test script
              runs: |
                cat <<EOF > SimpleJob.scala
                val data = Seq(1, 2, 3, 4, 5)
                val rdd = spark.sparkContext.parallelize(data)
                val sum = rdd.reduce(_ + _)
                assert(sum == 15)
                EOF
                cat SimpleJob.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy --master local[*]
            - name: Perform SQL query on DataFrame in Scala
              runs: |
                cat <<EOF > SQLTest.scala
                import org.apache.spark.sql.SparkSession
                val spark = SparkSession.builder.appName("SQLTest").getOrCreate()
                import spark.implicits._
                val df = Seq((1, "Alice"), (2, "Bob")).toDF("id", "name")
                df.createOrReplaceTempView("people")
                val result = spark.sql("SELECT name FROM people WHERE id = 2")
                assert(result.count() == 1 && result.first().getString(0) == "Bob")
                EOF
                cat SQLTest.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy --master local[*]
            - name: Run a simple Spark job in Python
              runs: |
                ## Ensure the user environment is properly configured
                if ! whoami &>/dev/null; then
                  echo "sparkuser:x:$(id -u):$(id -g):Spark User:/home/sparkuser:/bin/bash" >> /etc/passwd
                  mkdir -p /home/sparkuser
                  chown $(id -u):$(id -g) /home/sparkuser
                fi
                # Export necessary environment variables
                export USER=$(whoami)
                export LOGNAME=$(whoami)
                export HADOOP_USER_NAME=$(whoami)

                cat <<EOF > /tmp/simple_job.py
                from pyspark.sql import SparkSession
                spark = SparkSession.builder.appName("SimpleJob").config("spark.driver.bindAddress", "localhost").getOrCreate()
                data = [1, 2, 3, 4, 5]
                rdd = spark.sparkContext.parallelize(data)
                sum = rdd.reduce(lambda x, y: x + y)
                assert sum == 15
                EOF
                /usr/lib/spark/bin/spark-submit --jars /usr/lib/spark/jars/guava-33.2.1-jre.jar --conf spark.jars.ivy=/tmp/.ivy --master local[*] /tmp/simple_job.py

  - name: pyspark-scala-2.13
    description: Pyspark built with scala-2.13
    pipeline:
      - working-directory: python
        pipeline:
          - runs: |
              python3 setup.py sdist
          - uses: python/build
          - uses: python/install

  - name: ${{package.name}}-scala-2.13-compat
    description: "Compatibility package to place binaries in the location expected by upstream image"
    pipeline:
      - runs: |
          mkdir -p "${{targets.subpkgdir}}"/usr/bin
          mkdir -p "${{targets.subpkgdir}}"/opt
      - runs: |
          ln -s /usr/lib/spark/ ${{targets.subpkgdir}}/opt/spark
          ln -sf /usr/lib/spark/bin/spark-submit ${{targets.subpkgdir}}/usr/bin/spark-submit
          ln -sf /usr/lib/spark/bin/spark-shell ${{targets.subpkgdir}}/usr/bin/spark-shell
          ln -sf /usr/lib/spark/bin/spark-class ${{targets.subpkgdir}}/usr/bin/spark-class
          ln -sf /usr/lib/spark/bin/spark-sql ${{targets.subpkgdir}}/usr/bin/spark-sql
          ln -sf /usr/lib/spark/entrypoint.sh ${{targets.subpkgdir}}/opt/entrypoint.sh

update:
  enabled: true
  github:
    identifier: apache/spark
    use-tag: true
    strip-prefix: v
    tag-filter: v3.5
