package:
  name: spark-3.5
  version: 3.5.3
  epoch: 5
  description: Unified engine for large-scale data analytics
  copyright:
    - license: Apache-2.0

environment:
  contents:
    packages:
      - R
      - R-dev
      - bash
      - busybox
      - ca-certificates-bundle
      - curl
      - glibc-iconv
      - glibc-locale-en
      - grep
      - maven
      - openjdk-11
      - openjdk-17
      - openjdk-8
      - perl-utils
      - procps
      - py3.11-pip
      - python-3.11
      - wolfi-base
      - wolfi-baselayout
      - yaml-dev
  environment:
    LANG: en_US.UTF-8

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/apache/spark
      tag: v${{package.version}}
      expected-commit: 32232e9ed33bb16b93ad58cfde8b82e0f07c0970

  - uses: patch
    with:
      patches: make-distribution.patch internal-access.patch

  - uses: maven/pombump

data:
  - name: openjdk-versions
    items:
      8: "java-1.8-openjdk"
      11: "java-11-openjdk"
      17: "java-17-openjdk"

  - name: scala_versions
    items:
      1: 2.12
      2: 2.13

subpackages:
  - range: openjdk-versions
    name: ${{package.name}}-openjdk-${{range.key}}
    dependencies:
      # Both `python` and `openjdk-{11,17}-default-jvm` are required for running Spark,
      # but will be added in the image, since upstream supports different Python versions
      # and Java versions. So we don't need to specify them here to avoid conflicts.
      runtime:
        - openjdk-${{range.key}}-default-jvm
        - bash
        - busybox
        - procps
        - posix-libc-utils
        - tini
        - tini-compat
        - net-tools
        - logrotate
        - linux-pam
        - procps
        - libnss
      provides:
        - spark=${{package.full-version}}
        - ${{package.name}}=${{package.full-version}}
    pipeline:
      - runs: |
          export JAVA_HOME="/usr/lib/jvm/${{range.value}}"
          export PATH=$PATH:$JAVA_HOME/bin

          ./dev/make-distribution.sh --name custom-spark --pip -Psparkr -Phive -Phive-thriftserver -Pmesos -Pyarn -Pkubernetes

          patch dist/bin/load-spark-env.sh load-spark-env.sh.diff
          patch dist/sbin/spark-daemon.sh spark-daemon.sh.diff

          mkdir -p ${{targets.contextdir}}/usr/lib/spark
          mkdir -p ${{targets.contextdir}}/usr/lib/spark/work-dir
          mv dist/* ${{targets.contextdir}}/usr/lib/spark/
    test:
      environment:
        contents:
          packages:
            - python3
            - openjdk-${{range.key}}-default-jvm
            - R
        environment:
          LANG: en_US.UTF-8
          JAVA_HOME: /usr/lib/jvm/${{range.value}}
          SPARK_LOCAL_IP: 127.0.0.1
          SPARK_LOCAL_HOSTNAME: localhost
          SPARK_USER: tester
          HADOOP_USER_NAME: tester
          SPARK_HOME: /usr/lib/spark
          USERNAME: APP
      pipeline:
        - name: Test ${{package.name}} with OpenJDK ${{range.key}}
          pipeline:
            - name: Check spark-shell --version
            - runs: /usr/lib/spark/bin/spark-shell --version
            - name: Check spark-submit --version
            - runs: /usr/lib/spark/bin/spark-submit --version
            - name: Check pyspark --version
            - runs: /usr/lib/spark/bin/pyspark --version
            - name: Check spark-sql --version
            - runs: /usr/lib/spark/bin/spark-sql --version
            - name: Test entrypoint
              runs: timeout 10 /opt/entrypoint.sh /opt/spark/bin/spark-shell > spark_log.txt 2>&1 || [ $? -eq 143 ] && grep "Spark session available" spark_log.txt && exit_code=0 || exit_code=$? && echo $exit_code
            - name: Run a simple Scala test script
              runs: |
                cat <<EOF > SimpleJob.scala
                val data = Seq(1, 2, 3, 4, 5)
                val rdd = spark.sparkContext.parallelize(data)
                val sum = rdd.reduce(_ + _)
                assert(sum == 15)
                EOF
                cat SimpleJob.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy
            - name: Run a simple Spark job in Python
              runs: |
                cat <<EOF > simple_job.py
                from pyspark.sql import SparkSession
                spark = SparkSession.builder.appName("SimpleJob").getOrCreate()
                data = [1, 2, 3, 4, 5]
                rdd = spark.sparkContext.parallelize(data)
                sum = rdd.reduce(lambda x, y: x + y)
                assert sum == 15
                EOF
                /usr/lib/spark/bin/spark-submit --conf spark.jars.ivy=/tmp/.ivy simple_job.py --jars /usr/lib/spark/jars/guava-32.0.1-jre.jar
            - name: Perform SQL query on DataFrame in Scala
              runs: |
                cat <<EOF > SQLTest.scala
                import org.apache.spark.sql.SparkSession
                val spark = SparkSession.builder.appName("SQLTest").getOrCreate()
                import spark.implicits._
                val df = Seq((1, "Alice"), (2, "Bob")).toDF("id", "name")
                df.createOrReplaceTempView("people")
                val result = spark.sql("SELECT name FROM people WHERE id = 2")
                assert(result.count() == 1 && result.first().getString(0) == "Bob")
                EOF
                cat SQLTest.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy --deploy-mode client
            - name: Run basic SQL query with spark-sql
              runs: |
                export HADOOP_USER_NAME=tester
                export SPARK_LOCAL_IP=127.0.0.1
                export SPARK_LOCAL_HOSTNAME=localhost
                export SPARK_USER=tester
                export SPARK_HOME=/usr/lib/spark
                cat <<EOF >/tmp/db.sql
                CREATE OR REPLACE TEMP VIEW test AS SELECT 1 AS id;
                SELECT * FROM test;
                EOF
                /usr/lib/spark/bin/spark-sql \
                  -f /tmp/db.sql \
                  --master local[*] \
                  --conf spark.sql.catalogImplementation=in-memory \
                  --conf spark.jars.ivy=/tmp/.ivy \
                  --conf spark.hadoop.security.authentication=false \
                  --conf spark.hadoop.security.authentication=false --deploy-mode client
            - name: Run SparkR script
              runs: |
                cat <<EOF > sparkr_test.R
                library(SparkR)
                sparkR.session()
                df <- as.DataFrame(data.frame(id = c(1, 2), name = c("Alice", "Bob")))
                createOrReplaceTempView(df, "people")
                df2 <- sql("SELECT * FROM people WHERE id > 1")
                stopifnot(count(df2) == 1)
                EOF
                /usr/lib/spark/bin/spark-submit --conf spark.jars.ivy=/tmp/.ivy --master local[*] sparkr_test.R

  - range: scala_versions
    name: ${{package.name}}-${{range.value}}-compat
    description: "Compatibility package to place binaries in the location expected by upstream image"
    pipeline:
      - runs: |
          mkdir -p "${{targets.subpkgdir}}"/usr/bin
          mkdir -p "${{targets.subpkgdir}}"/opt/
      - runs: |
          ln -s /usr/lib/spark/ ${{targets.subpkgdir}}/opt/spark
          ln -sf /usr/lib/spark/kubernetes/dockerfiles/spark/entrypoint.sh ${{targets.subpkgdir}}/opt/entrypoint.sh
          ln -sf /usr/lib/spark/bin/spark-submit ${{targets.subpkgdir}}/usr/bin/spark-submit
          ln -sf /usr/lib/spark/bin/spark-shell ${{targets.subpkgdir}}/usr/bin/spark-shell
          ln -sf /usr/lib/spark/bin/spark-class ${{targets.subpkgdir}}/usr/bin/spark-class
          ln -sf /usr/lib/spark/bin/spark-sql ${{targets.subpkgdir}}/usr/bin/spark-sql
          if [[ "${{range.value}}" = "2.12" ]]; then
            ln -sf /usr/lib/spark/kubernetes/dockerfiles/spark/decom.sh ${{targets.subpkgdir}}/opt/decom.sh
            ln -sf /usr/lib/spark/bin/pyspark ${{targets.subpkgdir}}/usr/bin/pyspark
            ln -sf /usr/lib/spark/bin/sparkR ${{targets.subpkgdir}}/usr/bin/spark
          else
            ln -sf /usr/lib/spark/assembly/target/scala-2.13/jars ${{targets.contextdir}}/opt/spark/jars
          fi
      - uses: strip

  - range: openjdk-versions
    name: ${{package.name}}-minimal-openjdk-${{range.key}}
    dependencies:
      runtime:
        - openjdk-${{range.key}}-default-jvm
      provides:
        - spark-minimal=${{package.full-version}}
        - ${{package.name}}-minimal=${{package.full-version}}
    pipeline:
      - runs: |
          export JAVA_HOME="/usr/lib/jvm/${{range.value}}"
          export PATH=$PATH:$JAVA_HOME/bin

          ./dev/make-distribution.sh --name minimal-spark --pip -Psparkr -Phive

          mkdir -p ${{targets.contextdir}}/usr/lib/spark
          mv dist/* ${{targets.contextdir}}/usr/lib/spark/

  - name: ${{package.name}}-bitnami-compat
    description: Bitnami compat for spark 3.5
    pipeline:
      - uses: bitnami/compat
        with:
          image: spark
          version-path: 3.5/debian-12
      - runs: |
          mkdir -p ${{targets.subpkgdir}}/opt/bitnami/spark/
          mkdir -p "${{targets.subpkgdir}}"/opt/bitnami/spark/logs
          mkdir -p "${{targets.subpkgdir}}"/opt/bitnami/spark/tmp

          mkdir -p ${{targets.subpkgdir}}/opt/bitnami/spark/conf.default/
          mkdir -p ${{targets.subpkgdir}}/opt/bitnami/spark/conf/

          cp -r /home/build/melange-out/${{package.name}}-openjdk-17/usr/lib/spark/conf/* ${{targets.subpkgdir}}/opt/bitnami/spark/conf.default/
          # We copy conf instead of symlinking to avoid issues with the bitnami volume mount.
          cp -r /home/build/melange-out/${{package.name}}-openjdk-17/usr/lib/spark/conf/* ${{targets.subpkgdir}}/opt/bitnami/spark/conf/

          ln -s /usr/lib/spark/jars ${{targets.subpkgdir}}/opt/bitnami/spark/jars
          ln -s /usr/lib/spark/bin ${{targets.subpkgdir}}/opt/bitnami/spark/bin
          ln -s /usr/lib/spark/data ${{targets.subpkgdir}}/opt/bitnami/spark/data
          ln -s /usr/lib/spark/python ${{targets.subpkgdir}}/opt/bitnami/spark/python
          ln -s /usr/lib/spark/licenses ${{targets.subpkgdir}}/opt/bitnami/spark/licenses
          ln -s /usr/lib/spark/kubernetes ${{targets.subpkgdir}}/opt/bitnami/spark/kubernetes
          ln -s /usr/lib/spark/sbin ${{targets.subpkgdir}}/opt/bitnami/spark/sbin
          ln -s /usr/lib/spark/yarn ${{targets.subpkgdir}}/opt/bitnami/spark/yarn

  - name: ${{package.name}}-scala-2.13
    dependencies:
      runtime:
        - openjdk-17-default-jvm
    pipeline:
      - runs: |
          export JAVA_HOME=/usr/lib/jvm/java-17-openjdk
          export MAVEN_OPTS="-Xss64m -Xmx2g -XX:ReservedCodeCacheSize=1g"
          ## pom bump seem to mess with the ability to switch scala version
          ## the solution is to restore the original pom file at the package build stage
          cp pom.xml.orig pom.xml
          mvn clean
          ./dev/change-scala-version.sh 2.13
          ./build/mvn -DskipTests -Pscala-2.13 clean package
          mkdir -p ${{targets.contextdir}}/usr/lib/spark
          mkdir -p ${{targets.contextdir}}/usr/lib/spark/work-dir
          mv bin/ ${{targets.contextdir}}/usr/lib/spark
          mv sbin/ ${{targets.contextdir}}/usr/lib/spark
          mv target ${{targets.contextdir}}/usr/lib/spark
          mv assembly ${{targets.contextdir}}/usr/lib/spark
          cp resource-managers/kubernetes/docker/src/main/dockerfiles/spark/entrypoint.sh ${{targets.contextdir}}/usr/lib/spark/

test:
  environment:
    contents:
      packages:
        - openjdk-17-default-jvm
        - bash
        - ${{package.name}}-scala-2.13
    environment:
      LANG: en_US.UTF-8
      SCALA_VERSION: 2.13
      JAVA_HOME: /usr/lib/jvm/java-17-openjdk
      SPARK_LOCAL_IP: 127.0.0.1
      SPARK_LOCAL_HOSTNAME: localhost
      SPARK_HOME: /usr/lib/spark
      SPARK_USER: test
  pipeline:
    - name: Test ${{package.name}} with OpenJDK 17
      pipeline:
        - name: Test if the Scala versions are correct
          runs: ls /usr/lib/spark/assembly/target/scala-2.13/jars/scala-* | grep -q $SCALA_VERSION
        - name: Check spark-shell --version
          runs: /usr/lib/spark/bin/spark-shell --version
        - name: Check spark-submit --version
          runs: /usr/lib/spark/bin/spark-submit --version
        - name: Check pyspark --version
          runs: /usr/lib/spark/bin/pyspark --version
        - name: Check spark-sql --version
          runs: /usr/lib/spark/bin/spark-sql --version
        - name: Test entrypoint
          runs: timeout 10 /usr/lib/spark/bin/spark-shell > spark_log.txt 2>&1 || [ $? -eq 143 ] && grep "Spark session available" spark_log.txt && exit_code=0 || exit_code=$? && echo $exit_code
        - name: Run a simple Scala test script
          runs: |
            cat <<EOF > SimpleJob.scala
            val data = Seq(1, 2, 3, 4, 5)
            val rdd = spark.sparkContext.parallelize(data)
            val sum = rdd.reduce(_ + _)
            assert(sum == 15)
            EOF
            cat SimpleJob.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy --master local[*]
        - name: Perform SQL query on DataFrame in Scala
          runs: |
            cat <<EOF > SQLTest.scala
            import org.apache.spark.sql.SparkSession
            val spark = SparkSession.builder.appName("SQLTest").getOrCreate()
            import spark.implicits._
            val df = Seq((1, "Alice"), (2, "Bob")).toDF("id", "name")
            df.createOrReplaceTempView("people")
            val result = spark.sql("SELECT name FROM people WHERE id = 2")
            assert(result.count() == 1 && result.first().getString(0) == "Bob")
            EOF
            cat SQLTest.scala | /usr/lib/spark/bin/spark-shell --conf spark.jars.ivy=/tmp/.ivy --master local[*]

update:
  enabled: true
  github:
    identifier: apache/spark
    use-tag: true
    strip-prefix: v
    tag-filter: v3.5
