package:
  name: spark-4.0
  version: "4.0.0"
  epoch: 2
  description: Unified engine for large-scale data analytics
  copyright:
    - license: Apache-2.0
  dependencies:
    provider-priority: 0
    runtime:
      - merged-usrsbin
      - wolfi-baselayout

vars:
  debian-version: "12"
  java-version: "17" # https://issues.apache.org/jira/browse/SPARK-45315
  py-version: "3.11"
  scala-version: "2.13" # https://issues.apache.org/jira/browse/SPARK-45314

var-transforms:
  - from: ${{package.version}}
    match: ^(\d+\.\d+)\.\d+$
    replace: "$1"
    to: major-minor-version

environment:
  contents:
    packages:
      - R
      - R-dev
      - bash
      - busybox
      - ca-certificates-bundle
      - curl
      - glibc-iconv
      - glibc-locale-en
      - grep
      - maven
      - openjdk-${{vars.java-version}}-default-jdk
      - perl-utils
      - procps
      - protobuf-dev
      - py${{vars.py-version}}-pip
      - py${{vars.py-version}}-setuptools
      - python-${{vars.py-version}}
      - yaml-dev
  environment:
    APACHE_MIRROR: "https://repo.maven.apache.org/maven2/org"
    MAVEN_OPTS: "-Dmaven.wagon.httpconnectionManager.ttlSeconds=25 -Daether.connector.http.connectionMaxTtl=25"
    JAVA_OPTS: "-Xms4g -Xmx4g"
    JAVA_HOME: /usr/lib/jvm/default-jvm
    LANG: en_US.UTF-8

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/apache/spark
      expected-commit: fa33ea000a0bda9e5a3fa1af98e8e85b8cc5e4d4
      tag: v${{package.version}}

  - uses: maven/pombump

subpackages:
  - name: ${{package.name}}-scala-${{vars.scala-version}}
    dependencies:
      runtime:
        - bash
        - busybox
        - libnss
        - linux-pam
        - logrotate
        - merged-usrsbin
        - net-tools
        - openssl
        # OpenJDK and PySpark virt provides are temporary and will be removed when the image is updated
        - posix-libc-utils
        - procps
        - tini
        - wolfi-baselayout
      provides:
        - spark=${{package.full-version}}
        - ${{package.name}}=${{package.full-version}}
      provider-priority: 2130
    pipeline:
      - runs: |
          git stash
          mvn clean
      - uses: maven/pombump
      - uses: patch
        with:
          patches: mvn.patch
      - runs: |
          ./dev/change-scala-version.sh ${{vars.scala-version}}
      - runs: |
          ./dev/make-distribution.sh --name pyspark-${{vars.scala-version}} \
            --pip --tgz -Pscala-${{vars.scala-version}} -Phive -Phive-thriftserver -Pkubernetes -Pyarn \
            -T$(grep -c processor /proc/cpuinfo)

          mkdir -p ${{targets.contextdir}}/usr/lib/spark
          mv dist/kubernetes/dockerfiles/spark/entrypoint.sh ${{targets.contextdir}}/usr/lib/spark
          mv dist/* ${{targets.contextdir}}/usr/lib/spark/
          mv ${{targets.contextdir}}/usr/lib/spark/examples/jars/spark-examples_${{vars.scala-version}}-${{package.version}}.jar ${{targets.contextdir}}/usr/lib/spark/examples/jars/spark-examples.jar
    test:
      environment:
        contents:
          packages:
            - ${{package.name}}-scala-${{vars.scala-version}}-compat
            - R
            - openjdk-${{vars.java-version}}-default-jvm
            - python3
            - wait-for-it
        environment:
          SPARK_CONF: "--conf spark.ssl.enabled=true --conf spark.ssl.protocol=TLSv1.3 --conf spark.network.crypto.enabled=true --conf spark.authenticate=true --conf spark.authenticate.secret=AbCdEfG0%12345678NotReal --conf spark.io.encryption.enabled=true"
          SPARK_HOME: "/usr/lib/spark"
          JAVA_HOME: "/usr/lib/jvm/default-jvm"
      pipeline:
        - name: Spark utils version check
          runs: |
            spark-shell --version
            spark-submit --version
            pyspark --version
            spark-sql --version
        - name: Start and wait for Spark
          runs: |
            /opt/entrypoint.sh spark-shell ${SPARK_CONF} &> spark_output.txt &
            if ! wait-for-it localhost:4440; then
              cat ./spark_output.txt
              exit 1
            fi
        - name: Calculate sum
          runs: spark-submit ./sum.py --master spark://localhost:4440 ${SPARK_CONF}
        - name: Calculate avg salary
          runs: spark-submit ./salaries.py --master spark://localhost:4440 ${SPARK_CONF}

  - name: ${{package.name}}-scala-${{vars.scala-version}}-compat
    description: "Compatibility package to place binaries in the location expected by upstream image"
    pipeline:
      - runs: |
          mkdir -p ${{targets.contextdir}}/opt
          ln -s /usr/lib/spark ${{targets.contextdir}}/opt/spark
          ln -s /usr/lib/spark/kubernetes/dockerfiles/spark/decom.sh ${{targets.contextdir}}/opt/decom.sh
          ln -s /usr/lib/spark/entrypoint.sh ${{targets.contextdir}}/opt/entrypoint.sh

          mkdir -p ${{targets.contextdir}}/usr/bin
          for path in ${{targets.outdir}}/${{package.name}}-scala-${{vars.scala-version}}/usr/lib/spark/bin/*; do
            name=${path##*/}
            ln -s ../lib/spark/bin/$name ${{targets.contextdir}}/usr/bin/$name
          done
    dependencies:
      runtime:
        - merged-usrsbin
        - wolfi-baselayout

update:
  enabled: true
  github:
    identifier: apache/spark
    use-tag: true
    strip-prefix: v
    tag-filter: v4.0
